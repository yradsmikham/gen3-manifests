---
---
# Source: spark/templates/svc-spark.yaml
kind: Service
apiVersion: v1
metadata:
  name: spark-service
spec:
  selector:
    app: spark
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      name: http
    - protocol: TCP
      port: 9000
      targetPort: 9000
      name: hdfs
    - protocol: TCP
      port: 8030
      targetPort: 8030
      name: yarn-scheduler
    - protocol: TCP
      port: 8031
      targetPort: 8031
      name: yarn-resource-tracker
    - protocol: TCP
      port: 8032
      targetPort: 8032
      name: yarn-address-manager
    - protocol: TCP
      port: 22
      targetPort: 22
      name: ssl
    - protocol: TCP
      port: 7077
      targetPort: 7077
      name: spark-master
  type: NodePort
---

# Source: spark/templates/dpl-spark.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-deployment
  labels:
    release: development
  annotations:
    gen3.io/network-ingress: "tube"
spec:
  selector:
    # Only select pods based on the 'app' label
    matchLabels:
      app: spark
  revisionHistoryLimit: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: spark
        dbsheepdog: "yes"
        # GEN3_DATE_LABEL
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - spark
              topologyKey: "kubernetes.io/hostname"
      automountServiceAccountToken: false
      volumes:
      containers:
        - name: gen3-spark
          image: "quay.io/cdis/gen3-spark:2020.09"
          ports:
          - containerPort: 22
          - containerPort: 9000
          - containerPort: 8030
          - containerPort: 8031
          - containerPort: 8032
          - containerPort: 7077
          livenessProbe:
            tcpSocket:
              port: 9000
            initialDelaySeconds: 10
            periodSeconds: 30
          env:
          - name: DICTIONARY_URL
            value: https://rushdatadictionary.blob.core.windows.net/$web/datadictionary/andy-dd-test/schema.json
          - name: HADOOP_URL
            value: hdfs://0.0.0.0:9000
          - name: HADOOP_HOST
            value: 0.0.0.0
          volumeMounts:
          imagePullPolicy: Always
          resources:
            limits:
              cpu: 0.5
              memory: 2Gi
          command: ["/bin/bash" ]
          args: 
            - "-c"
            - |
              # get /usr/local/share/ca-certificates/cdis-ca.crt into system bundle
              ssh server sudo /etc/init.d/ssh start
              update-ca-certificates
              python run_config.py
              hdfs namenode -format
              hdfs --daemon start namenode
              hdfs --daemon start datanode
              yarn --daemon start resourcemanager
              yarn --daemon start nodemanager
              hdfs dfsadmin -safemode leave
              hdfs dfs -mkdir /result
              hdfs dfs -mkdir /jars
              hdfs dfs -mkdir /archive
              /spark/sbin/start-all.sh
              while true; do sleep 5; done

